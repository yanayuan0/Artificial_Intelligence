{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a1ababd",
   "metadata": {},
   "source": [
    "# Experiment with the MNIST data set\n",
    "\n",
    "MNIST is a data set composed of hand-written digits that have been pre-processed to make the problem easier.  It's considered a 'toy' dataset now, but it was the original dataset that a lot of the earlier CNN work was done using.  The images are originally from scans of zip-codes on letters, and were provided by the US Post Office (see https://en.wikipedia.org/wiki/MNIST_database for more details).\n",
    "\n",
    "**CUDA NOTE:** For this lab, we'll be using the Keras/Tensorflow deep learning library, which by default will try to use a GPU to accelerate the processing of your ANN via the CUDA toolchain.  That means that what computer you're running Jupyter on matters, since not all of our Lab machines have the same GPU.  It also means that if too many things are trying to use the GPU on the same machine at once, you'll run into problems (like running out of GPU memory).  Trying to run this code on your personal computer may go drastically slower, since there's a good chance you don't have a high-powered GPU along with a correctly configured driver/library stack to take advantage of it.\n",
    "\n",
    "As a result, it's good practice to only have one Python kernel active, and to shut down the current Python kernel (in the 'Kernel' menu) before trying to use Tensorflow commands in a different notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "877355c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-25 15:20:01.468025: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from aitk.utils import gallery, array_to_image\n",
    "from aitk.networks import Network\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3487cdf2",
   "metadata": {},
   "source": [
    "# Get the data\n",
    "* Download the data\n",
    "* Explore what you have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a17a9d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_x, train_y), (test_x, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6645228b-1a84-4d06-a9ce-45a8ebf8c0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b410ef7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa37e5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00271a40-aabe-4c71-bb3d-cf796c7f7ef7",
   "metadata": {},
   "source": [
    "### Examining images\n",
    "Let's look at a 'raw' vector, and then interpret it as them as images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27da68f6-545a-4922-a436-23859dfb67fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  84, 185, 159, 151,  60,  36,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0, 222, 254, 254, 254, 254, 241, 198,\n",
       "        198, 198, 198, 198, 198, 198, 198, 170,  52,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  67, 114,  72, 114, 163, 227, 254,\n",
       "        225, 254, 254, 254, 250, 229, 254, 254, 140,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  17,  66,\n",
       "         14,  67,  67,  67,  59,  21, 236, 254, 106,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,  83, 253, 209,  18,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,  22, 233, 255,  83,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 129, 254, 238,  44,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,  59, 249, 254,  62,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0, 133, 254, 187,   5,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   9, 205, 248,  58,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0, 126, 254, 182,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  75, 251, 240,  57,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         19, 221, 254, 166,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "        203, 254, 219,  35,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  38,\n",
       "        254, 254,  77,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  31, 224,\n",
       "        254, 115,   1,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 133, 254,\n",
       "        254,  52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  61, 242, 254,\n",
       "        254,  52,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 254,\n",
       "        219,  40,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 121, 254, 207,\n",
       "         18,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38bef398",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [array_to_image(train_x[i]) for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb35ef08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr style=\"padding: 1px\"><td style=\"text-align: center; padding: 1px\">0<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==\" alt=\"0\" title=\"0\"></img></td><td style=\"text-align: center; padding: 1px\">1<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA/0lEQVR4nGNgGHhgPP/vfCMccgbv/vz58xa7nNnjv3/ev/xjyYYpxWXz4M/fP6dC/vytgggwIUnOPCDDwMBgxHOQQRdD0tibkfFQKeOL85OYGLG5ZTOPd6UoA8Pfz2gOVlv69+WFEAj775+lKHLsm/58cBeWgUkeRpG0/PPHHs5Blzz2dx+C8//vEWTX+hj834SQ/Pf/ArLG0D/PJOHWt//dxYMqeR8u1/znoTsDquREKMtg6Z+1DKgg7O9DCKPo3d9FaHIMoX9+TjKQDd308O/95RaYkn/+PL3+58+fI03oUgwMMsf//Pn758/LiZhSDAwMkg1//v7pVcUqR1cAAKxwbkTVIzd2AAAAAElFTkSuQmCC\" alt=\"1\" title=\"1\"></img></td><td style=\"text-align: center; padding: 1px\">2<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1ElEQVR4nGNgGArA+YU6AwMDAwMTAwMDg10gqqTpGQaEpEMQihyTohwjgndnMYqk9L9FSDqZUE2dw3AbIaknjirJz7AbIenFiSInrsjwFCGpznAVWbJH/NZnCIuFgYGBgeE0XIbPI8aNofkDsqQQAwODPpOzDFs00/eTP1nOQlUyMjAwTEv/8IiBQY/xz7drJ88cfPlEkI0BoTProRUDA8OjjddOMDAwMKSJ3mPACVb+64QxmbBIb8AnyYBHklEVj+R/JjySDJb4jMVj5/b/OB1IJQAAg3ksR3QPgSAAAAAASUVORK5CYII=\" alt=\"2\" title=\"2\"></img></td><td style=\"text-align: center; padding: 1px\">3<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAnElEQVR4nGNgGPyg5u9/e1xyCV9+/7WDMJkwJOXZcRvq8ub3ZXkO7HI2T37/jsOlcfbfv3txyYn8/f3aCYecwtm/v+twacz4/XcHPw65gA+/D4rjMvTv37/zcRk6/ffv3+o45Azu/v69BpfGV79/H+HBJfn39+9IXHLz///9K4/Lxid/v/fgCHAGh99/76CLYcYnNskbx/ApoyoAAGeYO0QsY6cRAAAAAElFTkSuQmCC\" alt=\"3\" title=\"3\"></img></td><td style=\"text-align: center; padding: 1px\">4<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA1ElEQVR4nN3QPwtBYRQG8EMU0e0uZLIw+QKXRZlMGC0GX8CglE0pk0VxPwQmE5YrJYPVIjYMlImSwXNiMOi97319AM/6O6fzh+g/Y5hr5mrRNByseAZba4D7EnlSN8wy3uAYXJOwDEw0ohKwD9mtxehqRLQBCnZr8GPkJ/Ll79y0m37GiIjiK2AQsGMYiIbryyvjmZO20U9gAIcjTg43GhfethOROToO+En6xRUlZhnSjd+I6BY7xVIRY79w4XapR9IOSTWWYSWUqE0xlH771R7UrULefm5U2pxVCt0AAAAASUVORK5CYII=\" alt=\"4\" title=\"4\"></img></td></tr><tr style=\"padding: 1px\"><td style=\"text-align: center; padding: 1px\">5<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABD0lEQVR4nGNgGGSAEY3Py+Mt1vsTq1LF6Rf+/PkzCZuUxowvf/4+uPznhQaGFP+M93/+/Lkhr/rnjw2GZMKfP3/+3JRlQJJkgkuGMjA8WO36mAHJTBY4KzVt151XDAwM4ti9BQFzEcayoEjkcTP+12U4dhxTC5fp5r9////9+0QZQ4rV7PGfz09Wffrz53kpG5ocm9+fP7XWDEIX/vz58yecHVVf+58/WwQYRE///d649s+fHU6GhnA55o4/H7MEGUxP/LnhyMDnsfjjnz/34ZKZfz5FCHmu+vKnTpaBgYGBIXLLFlW45PM/X8/e+PPnTw0zFo+f//Pnz59NJSqovoZGNm+A0at5739h0Ta4AABroXIjERrLHgAAAABJRU5ErkJggg==\" alt=\"5\" title=\"5\"></img></td><td style=\"text-align: center; padding: 1px\">6<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAh0lEQVR4nGNgGGAw8f9leVxyCm///nFHFmBCYr8+hKYaWfLrQzySAvp4JLnkGBhMcbqo9u+fPzm4JBnQJJlQJJkYGZG5LCiS//7jdBAGIEGSiZHRDqfSv3/+/NHCpXMGAwNDGi7JG/hcwHDr79//yjh0Mlz9//8fLmMZZqHw0CSvXcdrKx0AAOciI63Ko1kqAAAAAElFTkSuQmCC\" alt=\"6\" title=\"6\"></img></td><td style=\"text-align: center; padding: 1px\">7<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABEklEQVR4nM2RMS+DURiFn/ullKXC1KWJyVId2ARBQpqUHyBRC0NjsPsPNktj0F9QEgYiIvEDJG3CYhEpMTBI2qEk5+YzfP1uuD6bwVnum3ve877n5IV/jLH8Vmittfao36fyuw8tWUmSahmPPJEUk5oGIOXIixIvNRMyNZewMZXLZQEyLame9pR6jN7iMDx9JFtevZTk+4mwdtuVdD2IN3Z0fRFmQmjvnHY9TeE+jnLs/gJXGWOMCYwxKyUXIC5u5svn78DmdrJRAIYkpwx8svizv2+5536j/UUZYfZMOYCR8pvUWXAeAWiOU+0AS5MhV9XD78pm71Kyz/sD/sqJA0nSXWOvkBAgXXlVvZL9Jd4f4xPJmHJ5CeNkqwAAAABJRU5ErkJggg==\" alt=\"7\" title=\"7\"></img></td><td style=\"text-align: center; padding: 1px\">8<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAc0lEQVR4nGNgGMyA1f4obkmRf88kkPlMqNIS+CQZGfBI/ufEI8lgjFPyz0cGZZySHw6jGoNuLF5JYXySfrgl9+Mz9hEDqzxOyT8MjOy43Xft3zTckhM+cuA0loHh/y88knwBuI199l0Dt85Dt77j1kktAADVQhZzhi0BcQAAAABJRU5ErkJggg==\" alt=\"8\" title=\"8\"></img></td><td style=\"text-align: center; padding: 1px\">9<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA7klEQVR4nM3QsUtCURiG8QdRFIKEoCHIGtouSM4SRn9CixE0REtjS4tu0tLm4tIS4tLeFqE0FNjukIqLDrchCBq85H1Pt6Gl7vGs4bed78cDHwcWZtodgMRcqxeHzu4y+Cg78UH39rJ0twJw+NbftvHF7AD0ov2f95+DplEGKGx8ZezwIuytwtKNnlKW5V6DXeBKY7vLD1UHzj91GqfksYlMt5pee55dW92RZPpSdyLfsoMw8PcKbckonGzFsDM6AbxHGakVL89yAKV3lT1v2T4WyDbMYC4AVOSvu2xzFNac4UDN2ObXxze5dYb/N9+FeFNxEamP7gAAAABJRU5ErkJggg==\" alt=\"9\" title=\"9\"></img></td></tr><tr style=\"padding: 1px\"><td style=\"text-align: center; padding: 1px\">10<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA6ElEQVR4nL2RMUtCYRSGX8MwSChsMRAiaeiGSNDUFDg0Obp0myNbosEf4GwFUX+hrSnwL9wpMA26DhfEsSHCJBoEeZyqm55v9UznnOf7vvf9zpHmH4lYvlM+eWrrZmSdOx0CUDIfybwB8HFo0uoXfeDaNvDMC5C3YaUF4Dm8ZzvAw2+ZjLPjYkFSYF3bDkfwT3PhD3qbi5KkC1Px/BucmrfRqpJ3Kw6zkhJ1og0XTEGYc8EG1KZ7a4++JGl9YIzvnu7BlvaOWnC5NA33A+g1P2H8ujyrdXUGAO/x5s8/a6m0dn0N7E3PMSZiA2Ge8jxMjgAAAABJRU5ErkJggg==\" alt=\"10\" title=\"10\"></img></td><td style=\"text-align: center; padding: 1px\">11<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAm0lEQVR4nGNgGN5AXqPx1av5WKVcpr3/9/fv3+tYpOac/Pfv38dpSRyYUsKz/r05Hawuh9XICX8n8mB3B1fjPT9/LMYxMDAwMLT9XY5LioHh/z8/nHIMJ/8+csUuY87GINTw96MmFinJs69jGBhE/v61wiL5/GsOAwNDy9+dfFgkK7/++/fv5r/7RlitLFn68uWrLerMuJ07tAEA5ps3qzSdZC8AAAAASUVORK5CYII=\" alt=\"11\" title=\"11\"></img></td><td style=\"text-align: center; padding: 1px\">12<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABDklEQVR4nM2RvS9DYRjFT3030TChi6TuYBJKkFgkNWlCYiHRWfwHBglmTf8AH4lRrIauV0fEjbSTNgaLRCQGHSiuH8N1w1tvN4MzPXl/z3lzTh7pnykiqXOpNhbLFG4l3R1f1C1k+ZZfWk8YzusB6aEkqTzYnZTm8j+dTtpx4sEYu4HdRhGW4Xncjtp2niBpZ6kDeFntsLKJN6CWbrXCXFDmfGPIAqfy919dcz0W3D86u/8OcNJkT5U5BVizQ7UUgD1J0m+/70mqmDC+uRgMzcOSf2bs9xXpkiT1bgNF87MjGIlK0a1H+KhOm3AF8FzXA6jO1MVIHIbnfs1Oho+RcGhfSFXmdSW3fNmg45/pE8oAf9wKlFhvAAAAAElFTkSuQmCC\" alt=\"12\" title=\"12\"></img></td><td style=\"text-align: center; padding: 1px\">13<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA+UlEQVR4nNWPoUtDYRTFTxC0CIJhQcS0l8aYGPSF8TBOhWX/gxXZUxSbQcOKzVWbJmGMLQ/TXDVtZbyhbGEWnwtazvEzCML7tq+teNLl/rjnngPMTV704mQ3b6w7UKotPq86LBvU2a7j0Cd16MwiFV1hrthcc7Gnz346uVn4m4rb5uHLAVfywPsQQHkdp7bp8qPRDnByHEnGfn1ADdLI1chJV52N5OERh5fw7jW+2wzUTcICeYFUg3F1MdOLq0nXcxJokwF88tp6WVENuZFCeJHCqSrGAN8m+7o0yH/YTXzSL8Wkxns2ArYmFEnaWX613xJ5Gwaz2P/QDwv6bXmT2FBqAAAAAElFTkSuQmCC\" alt=\"13\" title=\"13\"></img></td><td style=\"text-align: center; padding: 1px\">14<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAbElEQVR4nGNgGMSAccUnGZySXE/+paAIMCGxv91iEMMpyTCVQQO3pbL/fkji1MnAyOaHW/I/AxtuSTRAgiQjPsn/+I29hE/yLpkOYmBgxyfphVPy5TU8xv76zuCK29gLDDy4nadwPAO3JNUAAMpqE3FnHzNvAAAAAElFTkSuQmCC\" alt=\"14\" title=\"14\"></img></td></tr><tr style=\"padding: 1px\"><td style=\"text-align: center; padding: 1px\">15<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA5UlEQVR4nN3PMUuCURjF8aODg1AoOdjk+CbOTg0SRps0tNnukNHi1BcIHFwbaowgGl3EoJBGwaFFCsrKT6BBGPW/5CLF+3KfL+CZ7uV3H+5zpKVJLHJvJPL7eix4Xpbq1z8A38MIrPfG4ymuD8B72LZfAQjWgq036ITxBvg8LEo6hedMyHY+YLQpSWpDKzzYhfuyJKWrk8XpP3uDu6wk6Rgesp4iklSZ8XVgmBzULDv5dS4wLNFx1ON+S9ZwFym/rVzBkTGnPDxZy2ycM8xZeAnRhn9/FFZ1dmsNNnmxGkpldk3zZg5Mtl9waa8xrgAAAABJRU5ErkJggg==\" alt=\"15\" title=\"15\"></img></td><td style=\"text-align: center; padding: 1px\">16<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA+klEQVR4nGNgGMQgdsnlX78O8WOTEtnw9+2WLZ//XcMmeeZNuxADg8anP3WYcq5/lzMwMDAwNP27DxdjgjFY76xgYGBgYFjDwMGHoZODC0Kr//uXgaHzxzcIfe8agyqGJAz8/o1gY0iyczB8xnQuA9xOCwYGEecadQwpduWkf/8uzD3/8N/HBQwMjDBhTjFjcycGTi0GBoa/TxgWbH2L8C5n57V///79+/Dk179/s4zQjNv57/vmSc42Mgw3/t3hQbfr/z1DBgYGBpbOLy/Q9TEw/DvHwsDAwLH533cHTA/c+DdvY1PC1T/HDbF5r/n7z58/V3tgkxpEAACefFmwfI1N8AAAAABJRU5ErkJggg==\" alt=\"16\" title=\"16\"></img></td><td style=\"text-align: center; padding: 1px\">17<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA90lEQVR4nGNgGAKA+/RTBQYGBgYGFjQJKVGG947GN99iSOrmyjOoyTF0aDE+ZcMwLu/v37/fFj75+/dfDIZcw9e/8zpFGQxe/n3JgSHZ/e++JAODyqp/X7IwHWl+5e9CbqlNf98UYvEB++y/TwLu//2bi9V/vX///v33d5YsVsm8v3///tushlWOefW/f/82I4sgBcKKoP8MDP+x6pOq/vf39Jy/J7BKxv79V8kb+3ceNjmH9399OBRu/61DFmSC0q78h7b89eFnfINN8v///6wBE5nmTMNm7My/Kw/8/euL1TkMBX///nvTyIldUrDs835sAU4LAABuqmGniXwGlwAAAABJRU5ErkJggg==\" alt=\"17\" title=\"17\"></img></td><td style=\"text-align: center; padding: 1px\">18<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAw0lEQVR4nGNgoA9QWvlLA5ec1ae7/YI45Ly/93Ph0qf6dQcTLjmOAyf4cDqm+7sMTjn259txyjHUfjbCLXl4B24529+aDAwMDtpYJWdcYmdIePvvezY2yV8hDGz3Q3jDv3lgymn/C2CwmM7AwDD5IKak8z9NBl5hBgYGrb8wIRYk6ScMnyEUDCCCkpERyrD/jCn5/z+EZs1YjGmn5NNMBgYGBtY5V0WweCX7WyafYfzNi9JY5BgYsr/9/fuhiQ2r3OAAAO+sNJqmm38ZAAAAAElFTkSuQmCC\" alt=\"18\" title=\"18\"></img></td><td style=\"text-align: center; padding: 1px\">19<br/><img src=\"data:image/PNG;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAAw0lEQVR4nGNgGKZAKPPx379V2OUsj//98+fPn/nY5EQu/3kxw3P5n+tsWCSP/tnGwMCg+vqzPlSACUnyO8NGBgYGBoZPb6ACLEiSjIzvOZQTjF9EPcVi7Is/J07++ROC3bFXv/75++eTFnZJBouQv38W4JBjYND9+0cNicuEIqnDxMiAU/L7vwO/cJmqufkFDrcyMPA//FOM0zkz/yzBKefy9bM/LjmFt1+DcMlxTvmzEqehWX+PsOOSM3vSKINTI1YAAAjUQy/e2JANAAAAAElFTkSuQmCC\" alt=\"19\" title=\"19\"></img></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gallery(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d0fb80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4, 3, 5, 3, 6, 1, 7, 2, 8, 6, 9],\n",
       "      dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e6fa0a",
   "metadata": {},
   "source": [
    "# Prepare the data for the network\n",
    "* You may need to normalize the inputs so that they are in the range [0,1].\n",
    "* You may need to convert the targets so that they are represented as 'one-hot' vectors when you are doing categorization (this means a vector with the number of elements equal to the number of categories, where the $i$'th element being 1 means it belongs to the $i$'th class)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b3b9ff",
   "metadata": {},
   "source": [
    "###  Normalizing the input data\n",
    "\n",
    "Start by finding the minimum value and the maximum value within your data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdbfbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_input = train_x.min()\n",
    "max_input = train_x.max()\n",
    "print(\"range of input values is:\", min_input, max_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3220f9b",
   "metadata": {},
   "source": [
    "The formula for normalizing your data is:\n",
    "\n",
    "`(data - min_input)/(max_input - min_input)`\n",
    "\n",
    "For most image data the min_input is 0 and the max_input is 255, as is the case here.  This simplifies the equation to:\n",
    "\n",
    "`data/255`\n",
    "\n",
    "_NOTE:_ be cautious, since if you do this on a per-element basis, you can have problems with e.g. the corners of the images always having a value of 0; for images like this, we want to normalize using the same range for every element, but for other types of data other strategies may be more appropriate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe7b4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_normalized = train_x/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f37e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_normalized = test_x/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f08243",
   "metadata": {},
   "source": [
    "### Data sent into a Conv2D layer must have a depth\n",
    "* This may require you to do a reshape command.\n",
    "* For these black and white images there is only one channel of information.\n",
    "* For color images there are typically 3 channels (Red, Green, Blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b30a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_normalized = train_x_normalized.reshape(60000,28,28,1)\n",
    "test_x_normalized = test_x_normalized.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ebc277",
   "metadata": {},
   "source": [
    "### Target data\n",
    "\n",
    "When doing a classification task we would typically convert the output into one-hot vectors. For some data sets you may not know how many classes there are.  You can put the training data into a set to find out how many unique classifications you have. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e85c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categories = len(set(train_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef4f4cc",
   "metadata": {},
   "source": [
    "Then you can use the number of categories to produce one-hot vectors using `to_categorical`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1797f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_category = to_categorical(train_y, num_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2e5381",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y_category = to_categorical(test_y, num_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89cf86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_category[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba94c7a8",
   "metadata": {},
   "source": [
    "# Construct the model\n",
    "\n",
    "This is just one possible configuration of layers to learn the MNIST data set.  Feel free to experiment with the number of filters, the filter size, and the layers themselves.\n",
    "\n",
    "NOTE: In the final layer we are using an activation function called **softmax**.  This will output a pseudo-probability and is typically used in classification problems like the one we are trying to solve.  It should be paired with the loss function called **categorical_crossentropy**.\n",
    "\n",
    "Also, this will produce some output that may look like 'warnings', but it's actually just information being produced by tensorflow.  It's *possible* that there might be actual issues in there, but more often than not you can ignore those messages safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102de2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feed-forward network\n",
    "neural_net = Sequential()\n",
    "\n",
    "# add a convolutional layer, with 8 filters, each being 5x5; \n",
    "# uses the rectified linear activation function\n",
    "# the 'input_shape' should match the shape of one training vector\n",
    "# the 'name' is just for the humans looking at this, it doesn't impact the training process\n",
    "neural_net.add(Conv2D(8,(5,5),activation=\"relu\",name=\"conv1\",input_shape=(28,28,1)))\n",
    "\n",
    "# add a max-pooling layer with 2x2 \n",
    "neural_net.add(MaxPooling2D(pool_size=(2,2),name=\"pool1\"))\n",
    "\n",
    "# the \"flatten\" step just re-arranges things, so all the nodes are lined up like they would be in an MLP\n",
    "neural_net.add(Flatten(name=\"flatten\"))\n",
    "\n",
    "# add a fully-connected layer with 100 nodes using ReLU\n",
    "neural_net.add(Dense(100, activation='relu',name=\"hidden\"))\n",
    "\n",
    "# add a fully-connected output layer using the 'softmax' activation\n",
    "# the number of nodes should match the number of class labels (in this case, 10)\n",
    "neural_net.add(Dense(10, activation='softmax',name=\"output\"))\n",
    "\n",
    "# this just prints out some info about the network architecture we've specified\n",
    "neural_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0041929c",
   "metadata": {},
   "source": [
    "# Compile the model\n",
    "\n",
    "Keras uses the method 'compile' to indicate that the network architecture is fully specified, and we're ready to set things up to actually do processing.\n",
    "\n",
    "The 'optimizer' is what variant on gradient descent we'll be using, the 'loss' function is what defines the gradient, and the 'metrics' are what sort of scores we want it to report during the training process.\n",
    "\n",
    "For our XOR networks we defined loss as sum-squared error.  However, for categorical data like handwritten digits it is better to use a different loss function called *categorical_crossentropy* (again, this is the loss function that is designed to be paired with the *softmax* output-layer activation function).  This interprets the outputs as representing pseudo-probabilities and forces them to sum to 1.0.  Thus the output from the network will reflect how likely it considers a particular input to be a member of one of the output categories.\n",
    "\n",
    "The 'adam' optimizer is a version of SGD that uses ADAptive Momentum (hence the name), which means it's a bit less vulnerable to badly chosen hyperparameters like learning rate (here we'll leave things at their defaults, but you can specify lots of hyperparameters if you want, see https://keras.io/api/optimizers/adam/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50549d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\",\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5f7703",
   "metadata": {},
   "source": [
    "# Create an aitk Network\n",
    "This allows us to do more visualization of what is happening inside the network.  This is basically a front-end that we're adding on top of the Keras/Tensorflow based back-end, so it will take the network we've specified as its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15514aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network(neural_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d0d3ba",
   "metadata": {},
   "source": [
    "Each layer may create a different range of values.  Configure the layers to a typical range to ensure that the layers will be properly displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b41edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.set_config_layer(\"conv1\", colormap=(\"gray\", 0, 3))\n",
    "net.set_config_layer(\"pool1\", colormap=(\"gray\", 0, 3))\n",
    "net.set_config_layer(\"flatten\",colormap=(\"gray\", 0, 3))\n",
    "net.set_config_layer(\"hidden\", colormap=(\"gray\", 0, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edad9bf",
   "metadata": {},
   "source": [
    "The loop below shows you how the network performs on 10 sample images.  When you run this cell **prior** to training the output layer (at the top) has no clear winning categories before learning has occurred. However, if you re-run this cell **after** training you'll see that the network has correctly learned the classification for almost all of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0572451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dec388c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10,20):\n",
    "    net.propagate(test_x_normalized[i])\n",
    "    net.display(test_x_normalized[i])\n",
    "    sleep(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecf23e8",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023be008",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = net.fit(train_x_normalized, # training examples\n",
    "                  train_y_category,   # training labels\n",
    "                  verbose=1,          # verbose output\n",
    "                  validation_data=(test_x_normalized,  # validation examples\n",
    "                                   test_y_category),   # validation labels\n",
    "                  epochs=5)  # number of times to loop through the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d047bf77",
   "metadata": {},
   "source": [
    "# Inspect the feature maps\n",
    "\n",
    "We can ask the network to propagate to specific layers and inspect the representations created there to try to understand how it has solved the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bd0723",
   "metadata": {},
   "source": [
    "Using one test image, find out what the maximum value is so that you can set up the color map properly. If you see a large red box in one of the visualized channels below, then use the image number for that test image to correct the color map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1704d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "for layer in [\"conv1\", \"pool1\"]:\n",
    "    data = [net.propagate_to(test_x_normalized[49], layer, channel=channel)\n",
    "            for channel in range(8)]\n",
    "    largest = max([sublist.max() for sublist in data])\n",
    "    net.set_config_layer(layer, colormap=(\"gray\", 0, ceil(largest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef5c937",
   "metadata": {},
   "source": [
    "Check out 10 different test images to see how each channel is representing them. Press the **enter** key to toggle thru the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee52eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for test_case in range(40,50):\n",
    "    c_images = [net.propagate_to(test_x_normalized[test_case], \"conv1\", \"image\", channel=channel)\n",
    "                for channel in range(8)]\n",
    "    c_bigger = [image.resize((200,200),resample=0) for image in c_images]\n",
    "    original = test_x_normalized[test_case]\n",
    "    gallery([original] + c_bigger, labels=\"channel{index}\", gallery_shape=(9,1))\n",
    "    input(\"press enter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6753fe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_case in range(40,50):\n",
    "    p_images = [net.propagate_to(test_x_normalized[test_case], \"pool1\", \"image\", channel=channel)\n",
    "                for channel in range(8)]\n",
    "    p_bigger = [image.resize((200,200),resample=0) for image in p_images]\n",
    "    original = test_x_normalized[test_case]\n",
    "    gallery([original] + p_bigger, labels=\"channel{index}\", gallery_shape=(9,1))\n",
    "    input(\"press enter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fa1db7",
   "metadata": {},
   "source": [
    "# Examine the results\n",
    "Check which inputs the network is getting wrong. Recall that there are 10 thousand test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a2467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "outputs = net.predict(test_x_normalized)\n",
    "answers = [argmax(output) for output in outputs]\n",
    "targets = [argmax(target) for target in test_y_category]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d04fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect = [i for i in range(len(answers)) if answers[i] != targets[i]]\n",
    "missed_target = [targets[i] for i in incorrect]\n",
    "wrong_answer = [answers[i] for i in incorrect]\n",
    "print(\"Number of incorrectly categorized images\", len(missed_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195c3eb7",
   "metadata": {},
   "source": [
    "We can use the `Counter` class from the `collections` library to find out which target is most commonly missed and which wrong answer is most commonly given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2798ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de62ffb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ctr = Counter(missed_target)\n",
    "t_ctr.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1b200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ctr = Counter(wrong_answer)\n",
    "a_ctr.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54c007d-384c-4eb9-9211-e4c147782cf1",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "We can also build a **Confusion Matrix** to see what mistakes are being made.\n",
    "\n",
    "This is a matrix (or 2D array) where the 'true' labels are arranged on the rows, and the 'predictions' are arranged on the columns (or vice-versa if you give the argument in the other order).  The count in a given cell is how many times that prediction-target combination occurred (i.e. how many times that prediction was given when that target _should_ have been given).\n",
    "\n",
    "For more details on the library functions being used here, see:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229dd0e1-aaf1-457c-9fd3-2a9a1ab085a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(targets, answers)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1278883-349f-482c-9f86-e145ce3aef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's also options for pretter formatting; this one uses a color-based heat-map by default\n",
    "# there's a bunch of options to control the display in more detail if you want them\n",
    "cm_plt = ConfusionMatrixDisplay(cm)\n",
    "cm_plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94e61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [array_to_image(test_x[index]) for index in incorrect]\n",
    "gallery(images, labels=wrong_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f87a6ac-e36e-4120-9215-baa432b01e72",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Notice the patterns here, and think about whether they make sense.  \n",
    "\n",
    "You can look at either the rows or the columns, and they tell you slightly different things; one way is \"when the network predicts X, what is the most likely 'true' answer?\"  The other way is, \"if the true answer is Y, what predictions are the most likely?\"\n",
    "\n",
    "Sometimes things are fairly symmetric, but not always, so it's worth looking at both.\n",
    "\n",
    "For example, when I ran it I found that the number 9 tended to be confused with 4, 7, and 8; the number 7 tends to be confused with 2 and 9.\n",
    "\n",
    "Is this reasonable?  Is the pattern of mistakes the same if you re-train the network?\n",
    "\n",
    "Also note that based on the image visualization, some of the 'mistake' examples may look pretty silly to us, but others might be ambiguous even to a human (e.g. due to a bad scan).  How many of these errors are 'reasonable'?  Is it realistic to expect the network to do better than this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de39630-120b-4a0e-b2c4-cf30847d27dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
